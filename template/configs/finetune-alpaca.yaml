model:
  name: {{ product_name }}-{{ model_repository.split("/") | last }}
  description: "Fine-tune `{{ model_repository }}`."
  tags:
    - llm
    - lora
    - {{ model_repository }}
    - alpaca

settings:
  docker:
    parent_image: pytorch/pytorch:2.2.0-{{ cuda_version }}-cudnn8-runtime

steps:
  finetune:
    # Uncomment and set value to use a step operator for this step
    # step_operator:
    enable_step_logs: False
    parameters:
      config:
        base_model_repo: {{ model_repository }}
        from_safetensors: {{ from_safetensors }}
        precision: bf16-true
        quantize: bnb.nf4 # Enable quantization with 4-bit normal float
        {%- if huggingface_merged_model_repository %}
        merged_output_repo: {{ huggingface_merged_model_repository }}
        {%- else %}
        # OPTIONAL: Configure Huggingface repository to which the merged model should be pushed
        # merged_output_repo:
        {%- endif %}
        {%- if huggingface_adapter_model_repository %}
        adapter_output_repo: {{ huggingface_adapter_model_repository }}
        {%- else %}
        # OPTIONAL: Configure Huggingface repository to which the adapter should be pushed
        # adapter_output_repo:
        {%- endif %}
        training:
          save_interval: 1
          epochs: 5
          epoch_size: 50000
          global_batch_size: 128
          micro_batch_size: 4
          learning_rate: 3e-4
